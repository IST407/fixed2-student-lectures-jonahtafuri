{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will extend a base `Animal` class to create a more specific `Dog` class. This will help you practice inheritance and method overriding in Python.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Review the provided `Animal` class.\n",
    "2. Create a `Dog` class that inherits from `Animal`.\n",
    "3. Override the `make_sound` method to return \"Woof!\" instead of the generic animal sound.\n",
    "4. Add a new method called `fetch` that returns a string saying the dog's name and \"fetched the ball!\".\n",
    "\n",
    "Here's the base `Animal` class to start with:\n",
    "\n",
    "```python\n",
    "class Animal:\n",
    "    def __init__(self, name, species):\n",
    "        self.name = name\n",
    "        self.species = species\n",
    "    \n",
    "    def make_sound(self):\n",
    "        return \"Some generic animal sound\"\n",
    "\n",
    "    def describe(self):\n",
    "        return f\"{self.name} is a {self.species}\"\n",
    "```\n",
    "\n",
    "Your task is to create the `Dog` class below this `Animal` class definition.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The `Dog` class should inherit from `Animal`.\n",
    "- The `Dog` class should have its own `__init__` method that calls the parent class's `__init__` method.\n",
    "- Override the `make_sound` method to return \"Woof!\".\n",
    "- Implement a new `fetch` method as described above.\n",
    "\n",
    "#### Hint:\n",
    "\n",
    "Remember to use the `super()` function to call methods from the parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here\n",
    "#Answer here\n",
    "\n",
    "class Animal:\n",
    "    def __init__(self, name, species):\n",
    "        self.name = name\n",
    "        self.species = species\n",
    "    \n",
    "    def make_sound(self):\n",
    "        return \"Some generic animal sound\"\n",
    "\n",
    "    def describe(self):\n",
    "        return f\"{self.name} is a {self.species}\"\n",
    "    \n",
    "class Dog(Animal):\n",
    "    def __init__(self, name):\n",
    "        # Call the parent class's __init__ method\n",
    "        super().__init__(name, species=\"Dog\")\n",
    "    \n",
    "    def make_sound(self):\n",
    "        # Override the sound method for Dog\n",
    "        return \"Woof!\"\n",
    "    \n",
    "    def fetch(self):\n",
    "        return f\"{self.name} fetched the ball!\"\n",
    "    \n",
    "    def congratulate(self):\n",
    "        return f\"{self.name} is a good {self.species}\"\n",
    "\n",
    "# Create an instance of Dog and test it\n",
    "dog = Dog(\"Leo\")\n",
    "print(dog.describe()) \n",
    "print(dog.make_sound()) \n",
    "print(dog.fetch()) \n",
    "print(dog.congratulate()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create a `LogTransform` class that applies a logarithmic transformation to input data. This class should be compatible with scikit-learn's transformer interface and include an inverse transform method.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `LogTransform` class with the following methods:\n",
    "1. `__init__(self, base=np.e)`: Initialize the transformer with a logarithm base (default to natural log).\n",
    "2. `fit(self, X, y=None)`: This method should just return self (as log transform doesn't need fitting).\n",
    "3. `transform(self, X)`: Apply log transformation to the input data.\n",
    "4. `inverse_transform(self, X)`: Reverse the log transformation.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `TransformerMixin` from scikit-learn.\n",
    "- Handle potential errors, such as non-positive values in the input data.\n",
    "- Ensure that the `transform` and `inverse_transform` methods work correctly with both 1D and 2D numpy arrays.\n",
    "\n",
    "#### Hint:\n",
    "\n",
    "Remember to import necessary modules (numpy, and classes from scikit-learn). Use `np.log` and `np.exp` for the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (924882520.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, basemp.e): # e is euler's constant (for logarithmic transformation)\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install sklearn\n",
    "\n",
    "import pylance\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Standard-scaler - represents the value of where the number lies in normal distribution\n",
    "\n",
    "## Implement this class\n",
    "class LogTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, base = np.e): # e is euler's constant (for logarithmic transformation)\n",
    "        self.base = base\n",
    "\n",
    "    def fit(self, X, y=None): # no need to do anything here since we are doing log transfrom NOT NEEDED FOR ANYTHING\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        if np.any(X <= 0):\n",
    "            raise ValueError(\"Non positive values in sample\")\n",
    "        return np.log(X)\n",
    "    \n",
    "    def inverse_transfrom(self):\n",
    "        return np.power(self.base, X)\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create some sample data\n",
    "X = np.linspace(0.1, 10, 100).reshape(-1, 1)\n",
    "\n",
    "# Initialize and use the LogTransform\n",
    "log_transformer = LogTransform(base=np.e)\n",
    "X_transformed = log_transformer.fit_transform(X)\n",
    "X_inverse = log_transformer.inverse_transform(X_transformed)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(X, X)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(X, X_transformed)\n",
    "plt.title(\"Log Transformed Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Log(Y)\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(X, X_inverse)\n",
    "plt.title(\"Inverse Transformed Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify that inverse_transform reverses transform\n",
    "np.testing.assert_allclose(X, X_inverse)\n",
    "print(\"Transformation and inverse transformation verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a simple gradient descent algorithm to find the minimum of a quadratic function. This will help you understand the core concepts of gradient descent before applying it to linear regression.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Implement a function `quadratic(x)` that returns the value of the quadratic function f(x) = x^2 + 2x + 1.\n",
    "2. Implement a function `quadratic_derivative(x)` that returns the derivative of the quadratic function.\n",
    "3. Implement a `gradient_descent` function that uses these functions to find the minimum of the quadratic function.\n",
    "4. Visualize the progress of the gradient descent algorithm.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The `gradient_descent` function should take the following parameters:\n",
    "  - `start`: The starting point for x\n",
    "  - `learning_rate`: The step size for each iteration\n",
    "  - `num_iterations`: The number of iterations to run\n",
    "  - `tolerance`: Stop if the change in x is smaller than this value\n",
    "- Plot the quadratic function and show the path taken by gradient descent.\n",
    "- Print the final minimum point found by the algorithm.\n",
    "\n",
    "#### Hint:\n",
    "\n",
    "The derivative of x^2 + 2x + 1 is 2x + 2. The update rule for gradient descent is:\n",
    "x_new = x_old - learning_rate * derivative(x_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def quadratic(x):\n",
    "    return x^2 + 2*x + 1\n",
    "    # implement me\n",
    "\n",
    "def quadratic_derivative(x):\n",
    "    return 2*x + 2\n",
    "    # Implement me\n",
    "\n",
    "def gradient_descent(start, learning_rate, num_iterations, tolerance):\n",
    "    x = start\n",
    "    x_history = [x]\n",
    "    # Make predictions\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient = quadratic_derivative(x)\n",
    "        new_x = x - learning_rate * gradient\n",
    "        x_history.append(new_x)\n",
    "\n",
    "        if abs(new_x - x) <= tolerance:\n",
    "            x = new_x\n",
    "            return \n",
    "        # Update parameters\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    pass\n",
    "    # Should return x, x_history\n",
    "\n",
    "# Set parameters\n",
    "start = 2.0\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Run gradient descent\n",
    "minimum, x_history = gradient_descent(start, learning_rate, num_iterations, tolerance)\n",
    "\n",
    "# Prepare data for plotting\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = quadratic(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', label='f(x) = x^2 + 2x + 1')\n",
    "plt.plot(x_history, [quadratic(x) for x in x_history], 'ro-', label='Gradient Descent Path')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent on a Quadratic Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Add annotations\n",
    "for i, (x, y) in enumerate(zip(x_history, [quadratic(x) for x in x_history])):\n",
    "    if i % 5 == 0:  # Annotate every 5th point to avoid clutter\n",
    "        plt.annotate(f'Step {i}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The minimum point found is x = {minimum:.6f}\")\n",
    "print(f\"The value of f(x) at this point is {quadratic(minimum):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a Simple Linear Regression class that is compatible with scikit-learn's estimator interface. You'll use gradient descent for optimization, as discussed in the lecture notes.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `SimpleLinearRegression` class with the following methods:\n",
    "1. `__init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6)`: Initialize the model parameters.\n",
    "2. `fit(self, X, y)`: Fit the model to the training data using gradient descent.\n",
    "3. `predict(self, X)`: Make predictions using the trained model.\n",
    "4. `score(self, X, y)`: Calculate the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `RegressorMixin` from scikit-learn.\n",
    "- Use gradient descent to optimize the parameters (weight and bias).\n",
    "- Store the weight as `self.coef_` and the bias as `self.intercept_` (note the trailing underscores).\n",
    "- Implement early stopping in the `fit` method using the `tolerance` parameter.\n",
    "- Ensure that the `fit`, `predict`, and `score` methods work with both 1D and 2D numpy arrays for X.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- Remember to reshape input arrays if necessary to ensure consistent dimensions.\n",
    "- The gradient descent update rules for simple linear regression are:\n",
    "  - w = w - learning_rate * (1/n) * sum((y_pred - y) * x)\n",
    "  - b = b - learning_rate * (1/n) * sum(y_pred - y)\n",
    "- You can use `np.mean((y_true - y_pred) ** 2)` to calculate MSE for the stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleLinearRegression' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate and print R-squared score\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR-squared score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m(X_test,\u001b[38;5;250m \u001b[39my_test)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[0;32m     65\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X_test, y_test, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SimpleLinearRegression' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "\n",
    "\n",
    "w_history = []\n",
    "b_history = []\n",
    "mse_history = []\n",
    "class SimpleLinearRegression:\n",
    "    def __init__(self, learning_rate, n_iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.weights_history = []  # List to store weights at each iteration\n",
    "        self.bias_history = [] \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Here X is X_train and y is y_train\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent algorithm\n",
    "        for _ in range(self.n_iterations):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            #Storing list of weights and biases\n",
    "            self.weights_history.append(self.weights.copy())  # Copy to avoid reference issues\n",
    "            self.bias_history.append(self.bias)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "# Test the implementation\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SimpleLinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print R-squared score\n",
    "print(f\"R-squared score: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient: {model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will extend the Simple Linear Regression implementation from Exercise 4 to create a Multiple Linear Regression class. This class will also be compatible with scikit-learn's estimator interface and use gradient descent for optimization.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `MultipleLinearRegression` class with the following methods:\n",
    "1. `__init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6)`: Initialize the model parameters.\n",
    "  - the \"tolerance\" parameter is used as a stopping criterion for the gradient descent algorithm. It helps determine when the algorithm should stop iterating, based on how much the model parameters are changing between iterations.\n",
    "  - after each iteration in gradient descent, if the absolute value of the change in coefficients is less than tolerance, we can halt the gradient descent process\n",
    "2. `fit(self, X, y)`: Fit the model to the training data using gradient descent.\n",
    "3. `predict(self, X)`: Make predictions using the trained model.\n",
    "4. `score(self, X, y)`: Calculate the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `RegressorMixin` from scikit-learn.\n",
    "- Use gradient descent to optimize the parameters (weights and bias).\n",
    "- Store the weights as `self.coef_` and the bias as `self.intercept_` (note the trailing underscores).\n",
    "- Implement early stopping in the `fit` method using the `tolerance` parameter.\n",
    "- Ensure that the `fit`, `predict`, and `score` methods work with 2D numpy arrays for X.\n",
    "- Handle multiple features in the input data.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- The implementation will be very similar to the Simple Linear Regression class, but you'll need to handle multiple features.\n",
    "- The gradient descent update rules for multiple linear regression are:\n",
    "  - w = w - learning_rate * (1/n) * X.T.dot(y_pred - y)\n",
    "  - b = b - learning_rate * (1/n) * sum(y_pred - y)\n",
    "- You can use `np.mean((y_true - y_pred) ** 2)` to calculate MSE for the stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "\n",
    "class MultipleLinearRegression:\n",
    "    # implement me\n",
    "    pass\n",
    "\n",
    "# Test the implementation\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = MultipleLinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print R-squared score\n",
    "print(f\"R-squared score: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Plot results (for the first feature)\n",
    "plt.scatter(X_test[:, 0], y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test[:, 0], y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('X (first feature)')\n",
    "plt.ylabel('y')\n",
    "plt.title('Multiple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "\n",
    "# Compare with sklearn's LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model = LinearRegression()\n",
    "sk_model.fit(X_train, y_train)\n",
    "sk_score = sk_model.score(X_test, y_test)\n",
    "print(f\"sklearn LinearRegression R-squared score: {sk_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 6**\n",
    "\n",
    "In this exercise, you will compare the performance of your custom MultipleLinearRegression implementation with sklearn's LinearRegression. You'll examine the coefficients, intercept, and performance metrics while adjusting the learning rate of your custom implementation.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Generate a synthetic dataset using sklearn's `make_regression` function.\n",
    "2. Implement a function to train both your custom MultipleLinearRegression and sklearn's LinearRegression on the same data.\n",
    "3. Compare the coefficients, intercept, and R-squared scores of both models.\n",
    "4. Experiment with different learning rates for your custom model and observe how it affects the results.\n",
    "5. Create a plot showing the R-squared scores of your custom model for different learning rates.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- Use the MultipleLinearRegression class you implemented in Exercise 5.\n",
    "- Test at least 5 different learning rates for your custom model.\n",
    "- Create a plot comparing the performance (R-squared scores) of your custom model with different learning rates to sklearn's LinearRegression.\n",
    "- Print a comparison of coefficients and intercepts for the best performing custom model and sklearn's model.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- You can use `numpy.logspace` to generate a range of learning rates to test.\n",
    "- Consider using a validation set to select the best learning rate for your custom model.\n",
    "- Remember to reset your custom model before training with each new learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming you have your MultipleLinearRegression class from Exercise 5\n",
    "# from multiple_linear_regression import MultipleLinearRegression\n",
    "\n",
    "def compare_models(X_train, X_test, y_train, y_test, learning_rates):\n",
    "    ## Implement me\n",
    "    pass\n",
    "    # Should return sk_model, sk_score, custom_scores\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define learning rates to test\n",
    "learning_rates = np.logspace(-4, 0, 9)\n",
    "\n",
    "# Compare models\n",
    "sk_model, sk_score, custom_scores = compare_models(X_train, X_test, y_train, y_test, learning_rates)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(learning_rates, custom_scores, 'bo-', label='Custom Model')\n",
    "plt.axhline(y=sk_score, color='r', linestyle='--', label='sklearn Model')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('R-squared Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find best performing custom model\n",
    "best_lr_index = np.argmax(custom_scores)\n",
    "best_lr = learning_rates[best_lr_index]\n",
    "print(f\"Best learning rate for custom model: {best_lr:.6f}\")\n",
    "\n",
    "# Train best custom model\n",
    "best_custom_model = MultipleLinearRegression(learning_rate=best_lr, n_iterations=1000)\n",
    "best_custom_model.fit(X_train, y_train)\n",
    "\n",
    "# Compare coefficients and intercepts\n",
    "print(\"\\nCoefficients comparison:\")\n",
    "print(\"sklearn Model:\", sk_model.coef_)\n",
    "print(\"Custom Model:\", best_custom_model.coef_)\n",
    "\n",
    "print(\"\\nIntercept comparison:\")\n",
    "print(\"sklearn Model:\", sk_model.intercept_)\n",
    "print(\"Custom Model:\", best_custom_model.intercept_)\n",
    "\n",
    "print(\"\\nR-squared scores:\")\n",
    "print(\"sklearn Model:\", sk_score)\n",
    "print(\"Best Custom Model:\", custom_scores[best_lr_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will learn about scikit-learn pipelines and use them to combine your custom LogTransform class from Exercise 2 with your MultipleLinearRegression class from Exercise 5. This will demonstrate how your custom classes can be integrated into the scikit-learn ecosystem.\n",
    "\n",
    "#### Introduction to Pipelines\n",
    "\n",
    "Pipelines in scikit-learn are a way to chain multiple steps that can be cross-validated together while setting different parameters. They help in preventing data leakage between train and test sets and make your code cleaner and more modular.\n",
    "\n",
    "The `make_pipeline` function is a simple way to create a pipeline. It takes a series of estimators and returns a pipeline that chains them in sequence.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Create a pipeline that applies the LogTransform to the input features and then uses the MultipleLinearRegression for prediction.\n",
    "2. Compare the performance of this pipeline with a pipeline using sklearn's StandardScaler and LinearRegression.\n",
    "3. Use the pipelines on a dataset where a log transformation might be beneficial (e.g., data with exponential relationships in the features).\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- Use the LogTransform class you implemented in Exercise 2.\n",
    "- Use the MultipleLinearRegression class you implemented in Exercise 5.\n",
    "- Create two pipelines using `make_pipeline`:\n",
    "  a. Custom pipeline: LogTransform -> MultipleLinearRegression\n",
    "  b. sklearn pipeline: StandardScaler -> LinearRegression\n",
    "- Generate a synthetic dataset where log transformation of features could be beneficial.\n",
    "- Compare the R-squared scores of both pipelines.\n",
    "- Create scatter plots comparing the predictions of both pipelines against the true values.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- You can use `make_regression` with an exponential transformation on the features to create a dataset where log transformation might be useful.\n",
    "- Remember to handle any potential issues with non-positive values when applying the log transform.\n",
    "- Use scikit-learn's `make_pipeline` function to create the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming you have your LogTransform and MultipleLinearRegression classes from previous exercises\n",
    "# from log_transform import LogTransform\n",
    "# from multiple_linear_regression import MultipleLinearRegression\n",
    "\n",
    "# Generate synthetic data with exponential relationship in features\n",
    "n_samples, n_features = 1000, 5\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=42)\n",
    "X = np.exp(X)  # Apply exponential transformation to create non-linear relationship in features\n",
    "X = np.abs(X) + 1e-5  # Ensure all values are positive for log transform\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create custom pipeline\n",
    "custom_pipeline = make_pipeline(\n",
    "    #implment me\n",
    ")\n",
    "\n",
    "# Create sklearn pipeline\n",
    "sklearn_pipeline = make_pipeline(\n",
    "    #implment me\n",
    ")\n",
    "\n",
    "# Fit both pipelines\n",
    "custom_pipeline.fit(X_train, y_train)\n",
    "sklearn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "custom_pred = custom_pipeline.predict(X_test)\n",
    "sklearn_pred = sklearn_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate R-squared scores\n",
    "custom_r2 = r2_score(y_test, custom_pred)\n",
    "sklearn_r2 = r2_score(y_test, sklearn_pred)\n",
    "\n",
    "print(\"R-squared scores:\")\n",
    "print(f\"Custom Pipeline: {custom_r2:.4f}\")\n",
    "print(f\"sklearn Pipeline: {sklearn_r2:.4f}\")\n",
    "\n",
    "# Create scatter plots\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(y_test, custom_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Custom Pipeline: LogTransform + MultipleLinearRegression\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(y_test, sklearn_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"sklearn Pipeline: StandardScaler + LinearRegression\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare pipeline steps\n",
    "print(\"\\nCustom Pipeline steps:\")\n",
    "print(custom_pipeline.steps)\n",
    "\n",
    "print(\"\\nsklearn Pipeline steps:\")\n",
    "print(sklearn_pipeline.steps)\n",
    "\n",
    "# If you want to access the coefficients of the regression models:\n",
    "custom_coef = custom_pipeline.named_steps['multiplelinearregression'].coef_\n",
    "sklearn_coef = sklearn_pipeline.named_steps['linearregression'].coef_\n",
    "\n",
    "print(\"\\nRegression Coefficients:\")\n",
    "print(\"Custom Pipeline:\", custom_coef)\n",
    "print(\"sklearn Pipeline:\", sklearn_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
